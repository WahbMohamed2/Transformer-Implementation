{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75c8aaf8"
      },
      "source": [
        "### Import Libraries\n",
        "\n",
        "This cell imports the necessary libraries: `numpy` for numerical operations and `math` for mathematical functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZlsUgRCqcmi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42579c4a"
      },
      "source": [
        "### Initialize Variables\n",
        "\n",
        "This cell initializes the variables `L`, `d_k`, and `d_v` which represent the sequence length and the dimensions of the key and value vectors, respectively. It also creates random query, key, and value matrices (`q`, `k`, and `v`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L, d_k, d_v = 4, 8, 8\n",
        "q = np.random.randn(L, d_k)\n",
        "k = np.random.randn(L, d_k)\n",
        "v = np.random.randn(L, d_v)"
      ],
      "metadata": {
        "id": "GmCUxZnKumnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5691d1e2"
      },
      "source": [
        "### Print Q, K, and V\n",
        "\n",
        "This cell prints the initialized query (`q`), key (`k`), and value (`v`) matrices."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BJClsnlusK7",
        "outputId": "476e1f0d-1b2d-49c2-b0aa-d18728aa39de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[-0.23303838  0.4552888   0.95491234  0.30506956  0.32863433 -0.21465489\n",
            "  -1.99278369  0.6920726 ]\n",
            " [-0.66801934 -0.47584393  0.02423653 -2.02101804  0.660781   -0.64228401\n",
            "  -0.73995386  0.51579461]\n",
            " [-1.23958879 -1.38750281 -0.31920982 -1.50463355  1.96619006  1.4805927\n",
            "  -0.09300824 -1.00037272]\n",
            " [ 1.01917479  1.15913976  0.10872518 -0.1800475  -0.55870587  1.37986459\n",
            "   2.6599079   0.04387907]]\n",
            "K\n",
            " [[-0.96900161 -1.12374365 -1.26487243  0.53755025 -1.35861454  0.65570976\n",
            "  -1.15999351  0.12286737]\n",
            " [-0.06005638 -0.16036109 -0.74406779 -1.29358825 -0.56232546  0.49575063\n",
            "   0.877804   -0.32933588]\n",
            " [ 0.17975888  0.95998137  1.42153112 -1.41099863 -0.48974893  0.28141373\n",
            "   0.07628394  0.40003283]\n",
            " [ 2.07843739  1.33156505 -0.28815526 -1.24923639  1.09823374 -0.39904121\n",
            "   1.00280328 -0.77046439]]\n",
            "V\n",
            " [[-0.14568428  0.31482634  0.69393521  0.22385512  1.89562802  0.16136048\n",
            "   0.530136   -1.01611595]\n",
            " [-0.92017587 -0.75384648 -0.96106     0.22128155  2.16263508 -0.4784105\n",
            "   0.60419382 -1.71237406]\n",
            " [-0.02587124  1.53731072  0.636442    0.1246209   0.40339543  0.42203736\n",
            "  -0.20485709  2.02804858]\n",
            " [-1.99003486 -0.84010627  0.46174786 -0.49680789 -0.89477834 -0.22835937\n",
            "   0.5336981  -0.0690319 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0f52bcc"
      },
      "source": [
        "### Calculate Dot Product of Q and K Transpose\n",
        "\n",
        "This cell calculates the dot product of the query matrix (`q`) and the transpose of the key matrix (`k.T`). This is a crucial step in the self-attention mechanism."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.matmul(q,k.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8je8ob6cvBwm",
        "outputId": "4c75da2a-a8c3-4a4a-abf1-4ad8fb5e3055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.47974519, -3.43258167,  1.22564215, -2.6193899 ],\n",
              "       [-0.33219901,  1.20336642,  1.9547468 ,  0.33825211],\n",
              "       [ 0.63982505,  2.35701837, -0.83909208, -0.20633162],\n",
              "       [-3.94069403,  3.22358971,  2.58696108,  5.32469779]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fd83aef"
      },
      "source": [
        "### Calculate Variance of Q, K, and the Dot Product\n",
        "\n",
        "This cell calculates and prints the variance of the query matrix (`q`), the key matrix (`k`), and the result of the dot product of `q` and `k.T`. This helps to understand the spread of values in these matrices."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var(), k.var(), np.matmul(q,k.T).var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgepNLnCvTw1",
        "outputId": "a3e7e1b5-afe1-4ac7-d133-e3bb60781675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(1.1530857762373627),\n",
              " np.float64(0.857386828919557),\n",
              " np.float64(5.598960354561339))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8240e5f3"
      },
      "source": [
        "### Scale the Dot Product\n",
        "\n",
        "This cell scales the dot product of `q` and `k.T` by dividing it by the square root of `d_k`. This scaling is done to prevent the dot product from becoming too large, which can lead to vanishing gradients during training. It also prints the variance of the scaled matrix."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled = np.matmul(q,k.T) / math.sqrt(d_k)\n",
        "q.var(), k.var(), scaled.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YaEvHVjviJV",
        "outputId": "f72bfdb3-cd20-47d6-97c8-0e4f44dc72ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(1.1530857762373627),\n",
              " np.float64(0.857386828919557),\n",
              " np.float64(0.6998700443201673))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5ad88d"
      },
      "source": [
        "### Print Scaled Matrix\n",
        "\n",
        "This cell prints the scaled dot product matrix."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xmuTlbJvxp1",
        "outputId": "5ebdb2cc-7c18-4afb-d651-17e71297442e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.16961554, -1.21360089,  0.43332994, -0.92609418],\n",
              "       [-0.11745009,  0.42545428,  0.69110736,  0.11959018],\n",
              "       [ 0.22621232,  0.83333184, -0.29666385, -0.07294924],\n",
              "       [-1.39324573,  1.13971107,  0.91462886,  1.88256496]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cac626f"
      },
      "source": [
        "### Create a Lower Triangular Mask\n",
        "\n",
        "This cell creates a lower triangular mask using `np.tril` with dimensions `L x L`. This mask is used in masked self-attention to prevent attending to future tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.tril(np.ones(( L,L )))\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89qS-yx3v2pt",
        "outputId": "958f509d-f4ba-4261-af11-e8c50f45c0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1c1a9e5"
      },
      "source": [
        "### Modify the Mask\n",
        "\n",
        "This cell modifies the mask so that the lower triangle contains 0s and the upper triangle contains negative infinity (`-np.inf`). This is done so that when the mask is added to the scaled attention scores, the softmax function will output 0 for the masked values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask[mask == 0] = -np.inf\n",
        "mask[mask == 1] = 0\n",
        "\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIMhR6GNwJ3k",
        "outputId": "512fa749-ee4a-460f-f3af-e80367fa4693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ed16936"
      },
      "source": [
        "### Add Mask to Scaled Matrix\n",
        "\n",
        "This cell adds the modified mask to the scaled dot product matrix. The negative infinity values in the mask will effectively mask out the corresponding values in the scaled matrix."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask += scaled\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aatYZeZJwkBF",
        "outputId": "f24512a0-ea87-4956-c8ac-b8eb8c574881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.33923108,        -inf,        -inf,        -inf],\n",
              "       [-0.23490017,  0.85090856,        -inf,        -inf],\n",
              "       [ 0.45242463,  1.66666367, -0.5933277 ,        -inf],\n",
              "       [-2.78649147,  2.27942214,  1.82925772,  3.76512991]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94d13a13"
      },
      "source": [
        "### Define Softmax Function\n",
        "\n",
        "This cell defines a `softmax` function that takes a matrix `x` as input and applies the softmax function along the last axis. The softmax function is used to convert the attention scores into attention weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis =-1)).T"
      ],
      "metadata": {
        "id": "JNfqwXviwpPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b48ab44b"
      },
      "source": [
        "### Calculate Attention Weights\n",
        "\n",
        "This cell applies the `softmax` function to the masked and scaled dot product matrix to obtain the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention = softmax(scaled + mask)\n",
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l75RMORNyDeM",
        "outputId": "155cdfa9-28d1-414f-834f-541cf90118b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [1.64006731e-01, 8.35993269e-01, 0.00000000e+00, 0.00000000e+00],\n",
              "       [1.35344010e-01, 8.36459676e-01, 2.81963139e-02, 0.00000000e+00],\n",
              "       [4.64075221e-05, 9.26266314e-02, 4.71498140e-02, 8.60177147e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5da7e079"
      },
      "source": [
        "### Calculate New Value Matrix\n",
        "\n",
        "This cell calculates the final output of the self-attention layer by multiplying the attention weights with the value matrix (`v`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_v = np.matmul(attention, v)\n",
        "new_v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p3TmBipyL5M",
        "outputId": "9f2371e4-6907-473e-c37a-607c93fa0ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.14568428,  0.31482634,  0.69393521,  0.22385512,  1.89562802,\n",
              "         0.16136048,  0.530136  , -1.01611595],\n",
              "       [-0.79315404, -0.57857694, -0.68962964,  0.22170364,  2.11884413,\n",
              "        -0.37348375,  0.59204784, -1.59818304],\n",
              "       [-0.79013698, -0.54460583, -0.69202264,  0.2189044 ,  2.0768932 ,\n",
              "        -0.36643202,  0.57135829, -1.51267357],\n",
              "       [-1.79824188, -0.71996795,  0.33820553, -0.40095999, -0.55024229,\n",
              "        -0.22083659,  0.50540498, -0.12241615]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fleGyz5z-EL",
        "outputId": "4b321610-2a16-4b1f-f5c5-df2500687475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.14568428,  0.31482634,  0.69393521,  0.22385512,  1.89562802,\n",
              "         0.16136048,  0.530136  , -1.01611595],\n",
              "       [-0.92017587, -0.75384648, -0.96106   ,  0.22128155,  2.16263508,\n",
              "        -0.4784105 ,  0.60419382, -1.71237406],\n",
              "       [-0.02587124,  1.53731072,  0.636442  ,  0.1246209 ,  0.40339543,\n",
              "         0.42203736, -0.20485709,  2.02804858],\n",
              "       [-1.99003486, -0.84010627,  0.46174786, -0.49680789, -0.89477834,\n",
              "        -0.22835937,  0.5336981 , -0.0690319 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis =-1)).T\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled += mask\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention, v)\n",
        "  return out, attention"
      ],
      "metadata": {
        "id": "2FW_QsyN5wwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"New V\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1YTbRH66Pl5",
        "outputId": "36464f30-503e-4788-fbef-14fa5dd7f144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[-0.23303838  0.4552888   0.95491234  0.30506956  0.32863433 -0.21465489\n",
            "  -1.99278369  0.6920726 ]\n",
            " [-0.66801934 -0.47584393  0.02423653 -2.02101804  0.660781   -0.64228401\n",
            "  -0.73995386  0.51579461]\n",
            " [-1.23958879 -1.38750281 -0.31920982 -1.50463355  1.96619006  1.4805927\n",
            "  -0.09300824 -1.00037272]\n",
            " [ 1.01917479  1.15913976  0.10872518 -0.1800475  -0.55870587  1.37986459\n",
            "   2.6599079   0.04387907]]\n",
            "K\n",
            " [[-0.96900161 -1.12374365 -1.26487243  0.53755025 -1.35861454  0.65570976\n",
            "  -1.15999351  0.12286737]\n",
            " [-0.06005638 -0.16036109 -0.74406779 -1.29358825 -0.56232546  0.49575063\n",
            "   0.877804   -0.32933588]\n",
            " [ 0.17975888  0.95998137  1.42153112 -1.41099863 -0.48974893  0.28141373\n",
            "   0.07628394  0.40003283]\n",
            " [ 2.07843739  1.33156505 -0.28815526 -1.24923639  1.09823374 -0.39904121\n",
            "   1.00280328 -0.77046439]]\n",
            "V\n",
            " [[-0.14568428  0.31482634  0.69393521  0.22385512  1.89562802  0.16136048\n",
            "   0.530136   -1.01611595]\n",
            " [-0.92017587 -0.75384648 -0.96106     0.22128155  2.16263508 -0.4784105\n",
            "   0.60419382 -1.71237406]\n",
            " [-0.02587124  1.53731072  0.636442    0.1246209   0.40339543  0.42203736\n",
            "  -0.20485709  2.02804858]\n",
            " [-1.99003486 -0.84010627  0.46174786 -0.49680789 -0.89477834 -0.22835937\n",
            "   0.5336981  -0.0690319 ]]\n",
            "New V\n",
            " [[-0.14568428  0.31482634  0.69393521  0.22385512  1.89562802  0.16136048\n",
            "   0.530136   -1.01611595]\n",
            " [-0.79315404 -0.57857694 -0.68962964  0.22170364  2.11884413 -0.37348375\n",
            "   0.59204784 -1.59818304]\n",
            " [-0.79013698 -0.54460583 -0.69202264  0.2189044   2.0768932  -0.36643202\n",
            "   0.57135829 -1.51267357]\n",
            " [-1.79824188 -0.71996795  0.33820553 -0.40095999 -0.55024229 -0.22083659\n",
            "   0.50540498 -0.12241615]]\n",
            "Attention\n",
            " [[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.64006731e-01 8.35993269e-01 0.00000000e+00 0.00000000e+00]\n",
            " [1.35344010e-01 8.36459676e-01 2.81963139e-02 0.00000000e+00]\n",
            " [4.64075221e-05 9.26266314e-02 4.71498140e-02 8.60177147e-01]]\n"
          ]
        }
      ]
    }
  ]
}