{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "MLjkaLNSkeLU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These imports bring in:\n",
        "\n",
        "torch – the PyTorch library for tensor operations.\n",
        "\n",
        "math – used for mathematical constants and functions (e.g., square root).\n",
        "\n",
        "nn – PyTorch’s neural network module, where all layers and models come from.\n",
        "\n",
        "F – gives access to functions like softmax and activation operations."
      ],
      "metadata": {
        "id": "_c08VmzTkU-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n"
      ],
      "metadata": {
        "id": "D8Er3biNke2T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Compute attention scores between queries (Q), keys (K), and values (V).\n",
        "\n",
        "q, k, v shapes: [batch, heads, seq_len, head_dim].\n",
        "\n",
        "Scaling: Divide by √d_k to stabilize gradients.\n",
        "\n",
        "Masking: Adds large negative numbers (-inf) to prevent attending to future tokens.\n",
        "\n",
        "Softmax: Converts scores into probabilities.\n",
        "\n",
        "Weighted sum: Applies attention weights to values (V).\n",
        "\n",
        "Output: Returns the attended values and attention map"
      ],
      "metadata": {
        "id": "dbeREyaYkaor"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jVJwreSJkKIA"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applies two linear transformations with a ReLU in between.\n",
        "\n",
        "Expands from 512 → 2048 → 512.\n",
        "\n",
        "Adds nonlinearity and dropout for better generalization.\n",
        "\n",
        "Operates independently on each position (hence “position-wise”)."
      ],
      "metadata": {
        "id": "28rxUe-UknIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        mean = inputs.mean(dim=-1, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=-1, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "NB6l6X3Jk1_M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizes each feature across the last dimension (e.g., embedding size).\n",
        "\n",
        "Gamma and Beta allow rescaling and shifting after normalization.\n",
        "\n",
        "Prevents internal covariate shift, helping stabilize training."
      ],
      "metadata": {
        "id": "G-oZPM0hktS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.reshape(batch_size, seq_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "H79Wa35_k43D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projects input into queries, keys, and values.\n",
        "\n",
        "Splits them into multiple heads for parallel attention learning.\n",
        "\n",
        "Each head focuses on a different subspace of representation.\n",
        "\n",
        "Concatenates and linearly transforms them back to d_model dimension."
      ],
      "metadata": {
        "id": "Y7An46g8k4gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = nn.Linear(d_model, 2 * d_model)\n",
        "        self.q_layer = nn.Linear(d_model, d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, y, mask=None):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = kv.reshape(batch_size, seq_length, self.num_heads, 2 * self.head_dim)\n",
        "        q = q.reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
        "        kv = kv.permute(0, 2, 1, 3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k, v = kv.chunk(2, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.reshape(batch_size, seq_length, self.d_model)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Cx6wAD0yk3Ts"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used in Decoder–Encoder connections.\n",
        "\n",
        "K and V come from the encoder output (x),\n",
        "while Q comes from the decoder (y).\n",
        "\n",
        "Helps the decoder attend to relevant encoder features."
      ],
      "metadata": {
        "id": "O_0lO5oPlFl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = LayerNormalization([d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model, num_heads)\n",
        "        self.norm2 = LayerNormalization([d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
        "        self.norm3 = LayerNormalization([d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, decoder_mask):\n",
        "        _y = y\n",
        "        y = self.self_attention(y, mask=decoder_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.norm1(y + _y)\n",
        "\n",
        "        _y = y\n",
        "        y = self.encoder_decoder_attention(x, y)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.norm2(y + _y)\n",
        "\n",
        "        _y = y\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.norm3(y + _y)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "MRneLFQglHzr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flow:**\n",
        "\n",
        "**Masked Self-Attention:**\n",
        "The decoder looks at previous tokens (future ones masked).\n",
        "\n",
        "**Cross-Attention:**\n",
        "Connects to the encoder’s output for context.\n",
        "\n",
        "**Feedforward Network:**\n",
        "Adds nonlinearity and deeper representation power.\n",
        "\n",
        "**Residual + LayerNorm:**\n",
        "After each step, residual connections and normalization stabilize gradients."
      ],
      "metadata": {
        "id": "uvoxnlX3lJ6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, mask)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "drfakza6lCwD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stacks multiple DecoderLayers.**\n",
        "\n",
        "Feeds the output of one layer into the next in sequence."
      ],
      "metadata": {
        "id": "m8glIzprlTFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.layers = SequentialDecoder(*[\n",
        "            DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, y, mask):\n",
        "        y = self.layers(x, y, mask)\n",
        "        return y"
      ],
      "metadata": {
        "id": "0UZe0xsXlWnD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combines multiple layers into a full decoder stack.\n",
        "\n",
        "Each layer repeats self-attention → cross-attention → feedforward.\n",
        "\n",
        "num_layers defines decoder depth (here, 5 layers)."
      ],
      "metadata": {
        "id": "o20PSUsNlX2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5\n",
        "\n",
        "x = torch.randn((batch_size, max_sequence_length, d_model))\n",
        "y = torch.randn((batch_size, max_sequence_length, d_model))\n",
        "mask = torch.full([max_sequence_length, max_sequence_length], float('-inf'))\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "out = decoder(x, y, mask)"
      ],
      "metadata": {
        "id": "HdfbJcvelav8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_HU0EjwloHU",
        "outputId": "f25b71c5-88f7-41cc-e35f-c62ac22388bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.8483, -1.5130, -1.0294,  ..., -1.7971, -0.6627, -0.2493],\n",
            "         [-0.1286,  0.4190,  0.5515,  ..., -1.0953,  1.8280,  0.2260],\n",
            "         [-1.0160, -0.7728,  0.8552,  ..., -0.3770,  0.7206, -1.7009],\n",
            "         ...,\n",
            "         [-1.4675, -1.3779,  0.4696,  ..., -0.7159, -0.5289,  0.3690],\n",
            "         [-0.3545,  0.9258,  1.3856,  ...,  0.1064, -0.7601, -1.3119],\n",
            "         [ 0.5759, -0.3608, -0.6522,  ...,  0.1425, -0.3651,  0.8350]],\n",
            "\n",
            "        [[-0.8018, -1.6261, -1.2081,  ...,  0.1909,  0.0668,  1.0435],\n",
            "         [-2.7187,  0.7994,  0.7035,  ..., -0.6052,  0.3100, -0.4639],\n",
            "         [-0.3874,  2.0777,  1.1576,  ..., -0.2313, -1.9917,  1.4528],\n",
            "         ...,\n",
            "         [ 0.0914,  0.1942,  0.9081,  ..., -0.7985, -0.6961, -1.2718],\n",
            "         [ 0.8409, -0.4229,  1.5350,  ...,  0.2965, -1.4381, -0.6059],\n",
            "         [-1.8653,  0.6308, -0.3067,  ..., -0.0729, -0.1651,  0.2196]],\n",
            "\n",
            "        [[-0.0908, -0.2383,  0.1555,  ...,  0.5616,  0.1010,  0.6915],\n",
            "         [-0.2132, -1.6608, -1.1445,  ..., -0.6336,  0.4270, -0.9832],\n",
            "         [-0.1127,  1.0155,  0.6073,  ..., -0.1714, -0.1050,  0.2317],\n",
            "         ...,\n",
            "         [-0.0881,  0.0091,  1.8040,  ..., -0.3638,  0.1164,  1.0501],\n",
            "         [ 0.4530,  0.3484, -1.3165,  ...,  0.0488, -0.4969, -1.5548],\n",
            "         [ 0.3717,  0.1993, -0.5476,  ...,  0.1846, -0.7108,  1.5643]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.2608, -0.6519,  0.6667,  ...,  0.0753, -0.6564,  0.3058],\n",
            "         [ 0.8198, -0.0246, -0.6636,  ..., -1.3893, -0.3124,  1.1258],\n",
            "         [-1.3685,  0.8566,  0.9757,  ..., -0.6601, -0.4189, -0.1412],\n",
            "         ...,\n",
            "         [-0.8058, -0.4531,  0.6114,  ...,  0.9256, -2.2956, -1.1472],\n",
            "         [ 1.5920, -0.1390, -0.1323,  ...,  0.3896, -0.3162, -0.7781],\n",
            "         [-0.4178,  0.0949,  0.7093,  ..., -1.2966,  0.8887,  0.4949]],\n",
            "\n",
            "        [[ 0.6404, -1.6707, -0.3488,  ...,  0.4782,  1.0014, -1.2488],\n",
            "         [ 0.9062, -1.1340, -0.1050,  ..., -0.7903, -0.4157, -1.1410],\n",
            "         [ 0.5683,  1.2196,  1.3519,  ..., -0.0687, -0.1452,  1.4318],\n",
            "         ...,\n",
            "         [ 0.5921, -0.5280, -0.1094,  ..., -0.1817, -1.4081, -1.1779],\n",
            "         [-0.1219,  0.4039, -0.5824,  ..., -0.1116,  0.6317, -1.1309],\n",
            "         [-0.2255, -0.5092, -0.7150,  ...,  1.8919,  1.8734, -0.2381]],\n",
            "\n",
            "        [[-1.2193,  0.2291, -1.7858,  ..., -0.2026, -1.0508, -0.4118],\n",
            "         [ 0.1600, -0.6898,  2.0283,  ..., -0.6296,  0.9202,  0.9615],\n",
            "         [ 1.1990, -0.2096, -0.8472,  ..., -2.0150, -0.3118, -0.3059],\n",
            "         ...,\n",
            "         [-0.3509,  0.4322, -0.5473,  ..., -1.7692,  0.4191, -0.7605],\n",
            "         [-0.8132,  0.3595,  0.1959,  ...,  0.6084, -0.9672, -0.0043],\n",
            "         [ 1.4483, -0.0431,  0.2734,  ..., -0.0304, -0.9883, -1.1020]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}